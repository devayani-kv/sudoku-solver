{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cadda557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Input, Dropout\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78037670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    " # load dataset\n",
    " (trainX, trainY), (testX, testY) = mnist.load_data()\n",
    " # reshape dataset to have a single channel\n",
    " trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    " testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    " # one hot encode target values\n",
    " trainY = to_categorical(trainY)\n",
    " testY = to_categorical(testY)\n",
    " return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17fc0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_pixels(train, test):\n",
    " # convert from integers to floats\n",
    " train_norm = train.astype('float32')\n",
    " test_norm = test.astype('float32')\n",
    " # normalize to range 0-1\n",
    " train_norm = train_norm / 255.0\n",
    " test_norm = test_norm / 255.0\n",
    " # return normalized images\n",
    " return train_norm, test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aba39d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as k\n",
    "if k.image_data_format() == 'channels_first':\n",
    "  print('yes')\n",
    "else: print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf4ab5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  model = Sequential()\n",
    "  inputShape = (28,28,1)\n",
    "  classes = 10\n",
    "  model.add(Conv2D(32, (5, 5), padding=\"same\",input_shape=inputShape))\n",
    "  model.add(Activation(\"relu\"))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  # second set of CONV => RELU => POOL layers\n",
    "  model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
    "  model.add(Activation(\"relu\"))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  # first set of FC => RELU layers\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(64))\n",
    "  model.add(Activation(\"relu\"))\n",
    "  model.add(Dropout(0.5))\n",
    "  # second set of FC => RELU layers\n",
    "  model.add(Dense(64))\n",
    "  model.add(Activation(\"relu\"))\n",
    "  model.add(Dropout(0.5))\n",
    "  # softmax classifier\n",
    "  model.add(Dense(classes))\n",
    "  model.add(Activation(\"softmax\"))\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a5eaf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model():\n",
    "  INIT_LR = 1e-3\n",
    "  opt = Adam(lr=INIT_LR)\n",
    "  model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e68a30c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY, testX, testY = load_dataset()\n",
    "trainX, testX = prep_pixels(trainX, testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50595e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        832       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1568)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                100416    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 115,306\n",
      "Trainable params: 115,306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "compile_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6c52b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 43s 88ms/step - loss: 0.7103 - accuracy: 0.7628\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 40s 85ms/step - loss: 0.2594 - accuracy: 0.9247\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 39s 84ms/step - loss: 0.1971 - accuracy: 0.9420\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 0.1644 - accuracy: 0.9517\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 0.1430 - accuracy: 0.9578\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 37s 78ms/step - loss: 0.1315 - accuracy: 0.9606\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 36s 77ms/step - loss: 0.1184 - accuracy: 0.9659\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.1075 - accuracy: 0.9682\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 40s 85ms/step - loss: 0.1027 - accuracy: 0.9693\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 0.0914 - accuracy: 0.9723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24eba9d4a60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27a6d7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Devayani K\\sudoku-solver\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Devayani K\\sudoku-solver\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(r\"C:\\Users\\Devayani K\\sudoku-solver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13740fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(r\"C:\\Users\\Devayani K\\sudoku-solver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a6bea27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        832       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1568)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                100416    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 115,306\n",
      "Trainable params: 115,306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1281d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def identify_number(image):\n",
    "    image_resize = cv2.resize(image, (28,28))    # For plt.imshow\n",
    "    image_resize_2 = image_resize.reshape(1,28,28,1)    # For input to model.predict_classes\n",
    "#    cv2.imshow('number', image_test_1)\n",
    "    loaded_model_pred = loaded_model.predict(image_resize_2, verbose = 0)\n",
    "    class_x = np.argmax(loaded_model_pred,axis=1)\n",
    "\n",
    "    #prediction = model.predict(test_image, verbose=0)\n",
    "    #class_x = np.argmax(prediction,axis=1)\n",
    "    #print (\"Predicted digit: {}\".format(class_x))\n",
    "    return class_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bde69a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ime = []\n",
    "path = 'color_img1.jpg'\n",
    "sud = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "sudoku = cv2.resize(sud, (450,450))\n",
    "grid = np.zeros([9,9])\n",
    "for i in range(9):\n",
    "    for j in range(9):\n",
    "        image = sudoku[i*50:(i+1)*50,j*50:(j+1)*50]\n",
    "        ime.append(image)\n",
    "        if image.sum() > 100000:  #this line decides whether its a number or not\n",
    "            grid[i][j] = identify_number(image)\n",
    "        else:\n",
    "            grid[i][j] = 0\n",
    "grid =  grid.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a40b8884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 0, 0, 0, 1, 0, 0, 0, 9],\n",
       "       [0, 5, 0, 8, 0, 2, 0, 1, 0],\n",
       "       [0, 0, 4, 0, 9, 0, 2, 0, 0],\n",
       "       [0, 6, 0, 7, 0, 7, 0, 2, 0],\n",
       "       [5, 0, 8, 0, 6, 0, 1, 0, 2],\n",
       "       [0, 7, 0, 5, 0, 2, 0, 9, 0],\n",
       "       [0, 0, 2, 0, 4, 0, 6, 0, 0],\n",
       "       [0, 8, 0, 3, 0, 4, 0, 4, 0],\n",
       "       [3, 0, 0, 0, 5, 0, 0, 0, 8]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d142ef15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test images shape: (4, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "test_images = testX[17:21]\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28,1)\n",
    "print (\"Test images shape: {}\".format(test_images.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51b5a3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test images shape: (4, 28, 28, 1)\n",
      "Predicted digit: [7]\n",
      "Predicted digit: [3]\n",
      "Predicted digit: [4]\n",
      "Predicted digit: [9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"Test images shape: {}\".format(test_images.shape))\n",
    "for i, test_image in enumerate(test_images, start=1):\n",
    "    org_image = test_image\n",
    "    test_image = test_image.reshape(1,28,28,1)\n",
    "    prediction = model.predict(test_image, verbose=0)\n",
    "    class_x = np.argmax(prediction,axis=1)\n",
    "    print (\"Predicted digit: {}\".format(class_x))\n",
    "    plt.subplot(220+i)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Predicted digit: {}\".format(class_x))\n",
    "    plt.imshow(org_image, cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78543894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
